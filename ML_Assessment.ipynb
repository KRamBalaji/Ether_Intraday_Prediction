{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KRamBalaji/Ether_Intraday_Prediction/blob/main/ML_Assessment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORLuzQkceN2_"
      },
      "source": [
        "# Project -"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYQ2L_eheTgQ"
      },
      "source": [
        "## **1. Define Problem Statement-**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK3WNK9ksvK8"
      },
      "source": [
        "**Problem Statement:**\n",
        "\n",
        "> To predict IntraDay Ether (ETH) prices using machine learning, enabling informed trading decisions and potential profit maximization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRFnRnA8tsYk"
      },
      "source": [
        "### 1.1. Industry & Problem Type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt6iZKsQtCpU"
      },
      "source": [
        "\n",
        "\n",
        "* **Industry:** Cryptocurrency Trading/Financial Markets\n",
        "* **Problem Type:** This is primarily a **supervised learning** problem. We'll use historical Ether price data (labeled data) to train a model that can predict future prices. There might be elements of **time series analysis** involved as well due to the temporal nature of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7S_eL0Utova"
      },
      "source": [
        "### 1.2. Business Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I81URWQxtZUq"
      },
      "source": [
        "\n",
        "\n",
        "* **Why this problem?** Accurate prediction of Ether prices can provide a significant advantage in trading, allowing for better timing of buy and sell orders.\n",
        "* **Desired Outcome:** Develop a model capable of predicting IntraDay Ether prices with a certain level of accuracy (to be defined in the evaluation metrics section) to assist in trading decisions and potentially increase profitability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDpuFh_ztirf"
      },
      "source": [
        "### 1.3. Constraints & Limitations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCDAEnoztxai"
      },
      "source": [
        "* **Data Availability:** While historical Ether price data is generally available, obtaining high-quality, granular IntraDay data might require access to specific APIs or data providers.\n",
        "* **Computational Power:** Training complex machine learning models, especially with large datasets, may require significant computational resources. Depending on the chosen model and data size, we might need to consider cloud-based solutions like Google Colab for training.\n",
        "* **Obstacle:** The cryptocurrency market is highly volatile and influenced by various external factors. Achieving consistently accurate predictions can be challenging.\n",
        "* **Budget:** Depending on the data sources and computational resources required, there might be associated costs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ADaFXMFuBMf"
      },
      "source": [
        "### 1.4. Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59EnML1ZuEwn"
      },
      "source": [
        "* **Optimization Required:** We need to optimize the model for accuracy, precision, and potentially other metrics relevant to trading, such as minimizing false positives/negatives.\n",
        "* **KPIs Tracking:** Key performance indicators (KPIs) could include:\n",
        " * **Mean Absolute Error (MAE):** Measures the average absolute difference between predicted and actual prices.\n",
        " * **Root Mean Squared Error (RMSE):** Gives a higher weight to large errors.\n",
        " * **R-squared:** Indicates the proportion of variance in the dependent variable (price) explained by the model.\n",
        " * **Profitability/Returns:** Simulating trading strategies based on model predictions to assess potential gains.\n",
        "\n",
        "* **Required Testing:** We'll need to rigorously test the model on unseen data (a holdout set or through cross-validation) to ensure its generalization ability and avoid overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M6Mb3NjuiDW"
      },
      "source": [
        "### 1.5. Target Audience Relevancy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShPbq0HNum6-"
      },
      "source": [
        "* **Model Prediction Usage:** The primary target audience would be cryptocurrency traders or investors.\n",
        "* **Speed of Prediction:** For IntraDay trading, prediction speed is crucial. The model should be able to generate predictions quickly to enable timely trading decisions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWXVnkScuw63"
      },
      "source": [
        "### 1.6. Data Availability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prSkbIJSuzEW"
      },
      "source": [
        "* **Ease of Data Collection:** As mentioned earlier, historical Ether price data is readily available, but obtaining high-quality, granular IntraDay data might require some effort.\n",
        "* **Necessary Features:** Potential features could include:\n",
        " * Historical price data (Open, High, Low, Close)\n",
        " * Trading volume\n",
        " * Market sentiment (derived from social media or news articles)\n",
        " * Technical indicators (e.g., moving averages, RSI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faE2xX9Su09O"
      },
      "source": [
        "### 1.7. Scope of the Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MCWAfwTvGQk"
      },
      "source": [
        "* **Capabilities:** The solution aims to provide:\n",
        " * Accurate IntraDay Ether price predictions.\n",
        " * Potential trading signals (buy/sell) based on predictions.\n",
        " * Visualization of predicted prices and historical data.\n",
        "* **Expectations:** It's important to note that this solution is intended to be a tool to assist in trading decisions, not a guaranteed money-making machine. The cryptocurrency market is inherently unpredictable, and no model can perfectly predict future prices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAuZz7J1vWZp"
      },
      "source": [
        "### 1.8. Deployment Considerations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CemF3VzyveRr"
      },
      "source": [
        "* **Platform:** Google Colab could be used for development and initial deployment.\n",
        "Integration:\n",
        " * For wider accessibility, we could consider deploying the model as a web application or integrating it with a trading platform.\n",
        " * An API could be developed to provide predictions to other applications or systems.\n",
        "* **IntraDay Prices:** We'll need to ensure the data pipeline can continuously fetch and update IntraDay Ether prices to keep the model's predictions relevant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJgAcYVBvzdW"
      },
      "source": [
        "## **2. Data Collection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACcuWSx6wTee"
      },
      "source": [
        "### 2.1. Source Identification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2rV2JKrx0hA"
      },
      "source": [
        "* **Reliable Sources:** For this project, we can consider the following source:\n",
        " * **Kaggle:** Kaggle is an online community platform for data scientists and machine learning enthusiasts. It's owned by Google and serves several key purposes:\n",
        "   * **Datasets:** Kaggle provides a vast repository of publicly available datasets that users can explore, download, and use for their projects. You can find datasets on various topics, from image recognition to natural language processing.\n",
        "   *  **Notebooks:** Kaggle offers cloud-based Jupyter Notebooks, allowing users to write and execute code directly on the platform. These notebooks are integrated with GPUs and TPUs for accelerated computation.\n",
        "   * **Community:** Kaggle fosters a vibrant community of data scientists and machine learning practitioners. Users can share their work, collaborate on projects, ask questions, and learn from each other through forums and discussions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5tx00hyx07Y"
      },
      "source": [
        "### 2.2. Data Volume Required"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmcFBjW0x5eF"
      },
      "source": [
        "* **Sufficient Data:** The amount of data required depends on the complexity of the model and the desired accuracy. Generally, more data is better for training machine learning models, especially for time series analysis. For this project, aiming for at least a few years of historical IntraDay data (hourly or even more granular) would be ideal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMiViXDPx6Hs"
      },
      "source": [
        "### 2.3. Data Types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQu5vFnUx-eO"
      },
      "source": [
        "* **Labeled Data:** We'll be primarily working with labeled data, as historical Ether price data comes with timestamps and corresponding prices. These prices will serve as our target variable for training the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFsv0zieyBJH"
      },
      "source": [
        "### 2.4. Data Quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb5s9KAkyCnp"
      },
      "source": [
        "* **Data Cleaning:** Addressing data quality issues is crucial. We need to:\n",
        " * **Handle Missing Values:** Decide on a strategy for dealing with missing data points, such as imputation or removal.\n",
        " * **Outlier Detection and Treatment:** Identify and handle outliers that could skew the model's training.\n",
        " * **Data Consistency:** Ensure data is in a consistent format and units across all sources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yQMJWcnyEM6"
      },
      "source": [
        "### 2.5. Data Relevancy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2trvsO0QyGAB"
      },
      "source": [
        "* **Feature Selection:** We'll need to carefully select features that are relevant to Ether price prediction. This might involve:\n",
        " * **Historical Price Data:** Open, High, Low, Close prices.\n",
        " * **Trading Volume:** Indicator of market activity.\n",
        " * **Technical Indicators:** Moving averages, RSI, MACD, etc.\n",
        " * **Market Sentiment:** Data derived from social media, news articles, or dedicated sentiment analysis APIs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIuQx2ARyHPT"
      },
      "source": [
        "### 2.6. Temporal Considerations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njDdTVv0yJ1g"
      },
      "source": [
        "* **Time Series Nature:** Ether price data is inherently temporal, and we need to account for this:\n",
        " * **Seasonality:** Analyze for any seasonal patterns in price movements.\n",
        " * **Time-Based Features:** Consider incorporating time-based features like day of the week, hour of the day, etc., as potential predictors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i84Gl4PVyMf9"
      },
      "source": [
        "### 2.7. Legal and Ethical Concerns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cePpuDNbyPYV"
      },
      "source": [
        "* **Data Usage Rights:** Ensure you have the right to use the data from the chosen sources. Check API terms of service and website scraping policies.\n",
        "* **Privacy:** If incorporating personal or sensitive data (e.g., user data), ensure compliance with privacy regulations and data protection practices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLlz0a-FyRQb"
      },
      "source": [
        "### 2.8. Sampling Strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXYTSCHdyZdT"
      },
      "source": [
        "* **Sampling:** While having more data is generally better, sampling can be useful for initial model development or when dealing with massive datasets. You can consider techniques like random sampling or stratified sampling if appropriate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDqdkNGxyU7x"
      },
      "source": [
        "### 2.9. Data Privacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puNYrq_GygNB"
      },
      "source": [
        "* **Data Anonymization:** If using sensitive data, consider anonymization techniques to protect privacy. However, in this case, we'll primarily be working with publicly available market data, so this might not be a major concern."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ovmpkt_gycZU"
      },
      "source": [
        "### 2.10. Data Collection Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0GAAjICyTGm"
      },
      "source": [
        "* **APIs:** Use libraries like requests in Python to interact with exchange APIs and fetch data.\n",
        "* **Web Scraping:** Libraries like Beautiful Soup and Scrapy can be used for web scraping. However, exercise caution and respect website robots.txt rules.\n",
        "* **Data Handling:** Use libraries like pandas for data manipulation and storage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoMuYyXYylNJ"
      },
      "source": [
        "### 2.11. Data Versioning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRGmUMkaymzi"
      },
      "source": [
        "* **Version Control:** Implement version control using Git to track changes to your dataset and code. This is essential for reproducibility and collaboration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mYyPREYyqfm"
      },
      "source": [
        "### 2.12. Continuous Data Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eU7v26Rysi0"
      },
      "source": [
        "* **Data Pipelines:** Design a data pipeline to regularly fetch and update your dataset with fresh IntraDay Ether prices. This could involve scheduling scripts to run at specific intervals or using real-time data streams if available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HZ2To1z2Xqp"
      },
      "source": [
        "## **3. Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0JiDeI0r2zq5"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from numpy import loadtxt\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from matplotlib import rcParams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfOUH8D_20ai",
        "outputId": "81ce5476-5b84-490c-ca24-da7658e9ee25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Importing the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/ether_intraday_prices.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIa4kc6V26r8"
      },
      "outputs": [],
      "source": [
        "# Dataset First\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXMon0kx3EX0"
      },
      "source": [
        "### 3.1. Handling Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKrzTyAX3Hn9"
      },
      "outputs": [],
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSPa5huW9Xnt"
      },
      "source": [
        "### 3.2. Handling Outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyjbSlwCPShB"
      },
      "source": [
        "**Using Standard Deviation in Symmetric Curve**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "togf5EWk9f9n"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def find_outliers_sd(data, threshold=3):\n",
        "\n",
        "  # Calculate the mean and standard deviation\n",
        "  data_mean = np.mean(data)\n",
        "  data_std = np.std(data)\n",
        "\n",
        "  # Identify outliers\n",
        "  outliers = data[(data < data_mean - threshold * data_std) |\n",
        "                  (data > data_mean + threshold * data_std)]\n",
        "\n",
        "  return outliers\n",
        "\n",
        "# Example usage:\n",
        "# Assuming 'df' is your pandas DataFrame and 'column_name' is the column\n",
        "# containing the data you want to analyze:\n",
        "\n",
        "outliers = find_outliers_sd(df['Close'])\n",
        "\n",
        "# To see the output, run the code\n",
        "print(outliers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxpZoVAIPZTL"
      },
      "source": [
        "**Using IQR in skew-symmetric Curve**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arS0ACpE-nbU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def find_outliers_iqr(data):\n",
        "  # Calculate quantiles\n",
        "  q1 = np.quantile(data, 0.25)\n",
        "  q3 = np.quantile(data, 0.75)\n",
        "\n",
        "  # Calculate IQR\n",
        "  iqr = q3 - q1\n",
        "\n",
        "  # Define upper and lower bounds\n",
        "  upper_bound = q3 + 1.5 * iqr\n",
        "  lower_bound = q1 - 1.5 * iqr\n",
        "\n",
        "  # Identify outliers\n",
        "  outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
        "\n",
        "  return outliers\n",
        "\n",
        "# Assuming 'dataset' is your DataFrame and 'Close' is the column of interest:\n",
        "outliers = find_outliers_iqr(df['Close'])\n",
        "\n",
        "# Display the outliers\n",
        "print(outliers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o95FI30cPc0G"
      },
      "source": [
        "**Using Outlier Insensitive Algorithms.i.e. SVM, KNN, Decision Tree**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qo5LcV_7-ziU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR  # For SVM\n",
        "from sklearn.neighbors import KNeighborsRegressor  # For KNN\n",
        "from sklearn.tree import DecisionTreeRegressor  # For Decision Tree\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "X = df[['Open', 'High', 'Low', 'Volume']]  # Features\n",
        "y = df['Close']  # Target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 1. Support Vector Regression (SVR)\n",
        "svr_model = SVR(kernel='rbf')  # You can explore other kernels like 'linear', 'poly'\n",
        "svr_model.fit(X_train, y_train)\n",
        "svr_predictions = svr_model.predict(X_test)\n",
        "\n",
        "# 2. K-Nearest Neighbors (KNN)\n",
        "knn_model = KNeighborsRegressor(n_neighbors=5)  # Experiment with different 'n_neighbors'\n",
        "knn_model.fit(X_train, y_train)\n",
        "knn_predictions = knn_model.predict(X_test)\n",
        "\n",
        "# 3. Decision Tree Regression\n",
        "dt_model = DecisionTreeRegressor(random_state=42)  # You can adjust hyperparameters\n",
        "dt_model.fit(X_train, y_train)\n",
        "dt_predictions = dt_model.predict(X_test)\n",
        "\n",
        "# Evaluate models\n",
        "def evaluate_model(predictions, model_name):\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    r2 = r2_score(y_test, predictions)\n",
        "    print(f\"{model_name}:\")\n",
        "    print(f\"  Mean Squared Error (MSE): {mse}\")\n",
        "    print(f\"  R-squared (R2): {r2}\")\n",
        "\n",
        "evaluate_model(svr_predictions, \"Support Vector Regression (SVR)\")\n",
        "evaluate_model(knn_predictions, \"K-Nearest Neighbors (KNN)\")\n",
        "evaluate_model(dt_predictions, \"Decision Tree Regression\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxemcrAj_Txz"
      },
      "source": [
        "### 3.3. Categorical Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb07ReGC_bIc"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHE8KhYL_bjt"
      },
      "source": [
        "### 3.4. Data Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5K3_iHB__hDM"
      },
      "source": [
        "**Standardisation**\n",
        "\n",
        "This approach is based on the assumption that data follows a normal distribution, where most values cluster around the mean, and outliers lie further away."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_hobEzo_3Su"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming 'dataset' is your DataFrame and you want to standardize\n",
        "# the features 'Open', 'High', 'Low', and 'Volume'\n",
        "features_to_standardize = ['Open', 'High', 'Low', 'Volume']\n",
        "X = df[features_to_standardize]\n",
        "y = df['Close']  # Target variable\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data and transform it\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data using the fitted scaler\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Now, X_train_scaled and X_test_scaled contain the standardized features\n",
        "# You can use these scaled features for training your models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6UbuwXUHy1S"
      },
      "source": [
        "**Normalisation**\n",
        "\n",
        "This method scales your data to a specific range, typically between 0 and 1, which can be beneficial for certain machine learning algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92U8_0j-Hx2e"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def normalize_data(data):\n",
        "\n",
        "  # Create a MinMaxScaler object\n",
        "  scaler = MinMaxScaler()\n",
        "\n",
        "  # Fit the scaler to the data and transform it\n",
        "  normalized_data = scaler.fit_transform(data)\n",
        "\n",
        "  # If input was a pandas DataFrame, convert the output back to a DataFrame\n",
        "  if isinstance(data, pd.DataFrame):\n",
        "    normalized_data = pd.DataFrame(normalized_data, columns=data.columns, index=data.index)\n",
        "\n",
        "  return normalized_data\n",
        "\n",
        "\n",
        "columns_to_normalize = ['Open', 'High', 'Low', 'Volume']\n",
        "df[columns_to_normalize] = normalize_data(df[columns_to_normalize])\n",
        "\n",
        "# To see the output, run the code.\n",
        "# print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jVaoP6ZMdiQ"
      },
      "source": [
        "**Robust Scaler**\n",
        "\n",
        "a data transformation technique that's particularly useful when your data contains outliers. Unlike standard scaling (using mean and standard deviation), the Robust Scaler is less sensitive to extreme values.\n",
        "\n",
        "**How it Works:**\n",
        "\n",
        "The Robust Scaler removes the median and scales the data according to the quantile range (IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th percentile) and the 3rd quartile (75th percentile) of the data. This makes it robust to outliers, as the scaling is based on the more stable IQR rather than the potentially skewed mean and standard deviation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGaBIRFnIKfC"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "def robust_scale_data(data):\n",
        "\n",
        "  # Create a RobustScaler object\n",
        "  scaler = RobustScaler()\n",
        "\n",
        "  # Fit the scaler to the data and transform it\n",
        "  scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "  # If input was a pandas DataFrame, convert the output back to a DataFrame\n",
        "  if isinstance(data, pd.DataFrame):\n",
        "    scaled_data = pd.DataFrame(scaled_data, columns=data.columns, index=data.index)\n",
        "\n",
        "  return scaled_data\n",
        "\n",
        "\n",
        "columns_to_scale = ['Open', 'High', 'Low', 'Volume']\n",
        "df[columns_to_scale] = robust_scale_data(df[columns_to_scale])\n",
        "\n",
        "# To see the output, run the code\n",
        "# print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqypKB6iMiSs"
      },
      "source": [
        "**Sum of (median-observation)/IQR**\n",
        "\n",
        "* This method is a form of robust scaling that aims to center and scale your data using the median and the Interquartile Range (IQR), making it less sensitive to outliers.\n",
        "\n",
        "\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "\n",
        "\n",
        ">` X_scaled = (X - X_median) / IQR`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTOXuIhOMk8S"
      },
      "outputs": [],
      "source": [
        "def robust_scale_data_custom(data):\n",
        "\n",
        "    data_median = np.median(data)\n",
        "    q1 = np.quantile(data, 0.25)\n",
        "    q3 = np.quantile(data, 0.75)\n",
        "    iqr = q3 - q1\n",
        "\n",
        "    scaled_data = (data - data_median) / iqr\n",
        "\n",
        "    return scaled_data\n",
        "\n",
        "columns_to_scale = ['Open', 'High', 'Low', 'Volume']\n",
        "df['column_name_scaled'] = robust_scale_data_custom(df['Close'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CNKGQWKQXts"
      },
      "source": [
        "**Box-Cox Transformation**\n",
        "\n",
        "a powerful technique for transforming non-normal dependent variables into a more normal shape.\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "The Box-Cox transformation aims to stabilize variance and make the data more closely resemble a normal distribution. This is often desirable because many statistical methods assume normality for optimal performance.\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "The Box-Cox transformation is defined by the following formula:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "T(y) = (y^λ - 1) / λ   if λ != 0\n",
        "T(y) = ln(y)          if λ = 0\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOL0pslGP8xI"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "\n",
        "# Perform Box-Cox transformation\n",
        "transformed_data, lambda_value = stats.boxcox(df['Close'])\n",
        "\n",
        "# Add the transformed data to the DataFrame\n",
        "df['transformed_dependent_variable'] = transformed_data\n",
        "\n",
        "# To see the output, run the code.\n",
        "# print(df.head())\n",
        "# print(f\"Lambda value: {lambda_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2YuuhhjQmuI"
      },
      "source": [
        "**Gaussian Transformation**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "\n",
        "column_to_transform = 'Close'\n",
        "\n",
        "# Create a QuantileTransformer object with 'normal' output distribution\n",
        "qt = QuantileTransformer(output_distribution='normal', random_state=42)\n",
        "\n",
        "# Fit and transform the data\n",
        "df['column_name_gaussian'] = qt.fit_transform(df[[column_to_transform]])"
      ],
      "metadata": {
        "id": "GOt3i5UkeJ6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logarithmic Transformation**"
      ],
      "metadata": {
        "id": "eVP_r5UiephS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "column_to_transform = 'Close'\n",
        "\n",
        "# Apply logarithmic transformation using NumPy's log function\n",
        "df[column_to_transform + '_log'] = np.log(df[column_to_transform])\n",
        "\n",
        "# Handle potential errors (e.g., log of 0 or negative values)\n",
        "# You might need to add a small constant to avoid log(0) errors\n",
        "df[column_to_transform + '_log'] = np.log(df[column_to_transform] + 1e-6)  # Example"
      ],
      "metadata": {
        "id": "Qaf-1HQGeuAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inverse Transformation**"
      ],
      "metadata": {
        "id": "OPFgA6CMe8mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Close_log_inverse'] = np.exp(df['Close_log'])"
      ],
      "metadata": {
        "id": "ljzTbVeBfC25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Square Root Transformation**"
      ],
      "metadata": {
        "id": "sopU7R8OfetH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Close_sqrt'] = np.sqrt(df['Close'])"
      ],
      "metadata": {
        "id": "B6hG9Gu7filV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exponential Transformation**"
      ],
      "metadata": {
        "id": "4m_xy0HTgZBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the 'Close' column\n",
        "df['Close_scaled'] = scaler.fit_transform(df[['Close']])\n",
        "\n",
        "# Apply exponential transformation to the scaled column\n",
        "df['Close_scaled_exp'] = np.exp(df['Close_scaled'])"
      ],
      "metadata": {
        "id": "lcu8c8Ltgav9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "z9kY9ShNhRL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understanding Imbalanced Datasets**\n",
        "\n",
        "An imbalanced dataset occurs when one class (or category) has significantly more instances than another. In your case, if you're trying to predict price movements (e.g., up or down), you might find that one direction is much more common than the other in your historical data. This imbalance can lead to biased models that perform poorly on the minority class."
      ],
      "metadata": {
        "id": "tPy6sGgMjf0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross-Validation**"
      ],
      "metadata": {
        "id": "gTp5FIHvjiE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LinearRegression  # Example model\n",
        "from sklearn.metrics import mean_squared_error  # Example metric\n",
        "\n",
        "# 1. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Create your model\n",
        "model = LinearRegression()  # Example: using Linear Regression\n",
        "\n",
        "# 3. Perform cross-validation\n",
        "scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# 4. Evaluate the results\n",
        "# Convert negative MSE scores to positive\n",
        "mse_scores = -scores\n",
        "print(\"Cross-validation MSE scores:\", mse_scores)\n",
        "print(\"Average MSE:\", mse_scores.mean())\n",
        "\n",
        "# 5. Train the final model on the entire training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 6. Evaluate the final model on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "mse_test = mean_squared_error(y_test, y_pred)\n",
        "print(\"Test MSE:\", mse_test)"
      ],
      "metadata": {
        "id": "pbFdrbZlhh3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Under Sampling**"
      ],
      "metadata": {
        "id": "Tbo1NZk-j4iH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imblearn\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Discretize the target variable 'y' (e.g., using qcut)\n",
        "num_classes = 3  # Choose the desired number of classes\n",
        "y_classes = pd.qcut(y, q=num_classes, labels=False)\n",
        "\n",
        "# 1. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_classes, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Create a RandomUnderSampler object\n",
        "undersampler = RandomUnderSampler(random_state=42)  # You can adjust the random_state\n",
        "\n",
        "# 3. Apply undersampling to the training data\n",
        "X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train, y_train)\n",
        "\n",
        "# 4. Now you can train your model using the resampled data\n",
        "# ... (e.g., model.fit(X_train_resampled, y_train_resampled))"
      ],
      "metadata": {
        "id": "6YTK2SvykFt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Over Sampling**"
      ],
      "metadata": {
        "id": "GhtMKhR-k54K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Discretize the target variable 'y' (e.g., using qcut)\n",
        "num_classes = 3  # Choose the desired number of classes\n",
        "y_classes = pd.qcut(y, q=num_classes, labels=False)\n",
        "\n",
        "# 1. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_classes, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Create a SMOTE object\n",
        "oversampler = SMOTE(random_state=42)  # You can adjust the random_state and other parameters\n",
        "\n",
        "# 3. Apply oversampling to the training data\n",
        "X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
        "\n",
        "# 4. Now you can train your model using the resampled data\n",
        "# ... (e.g., model.fit(X_train_resampled, y_train_resampled))"
      ],
      "metadata": {
        "id": "_83JPfQ2k3F7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Synthetic Minority over Sampling Technique (SMOTE)**"
      ],
      "metadata": {
        "id": "qGIDuTDGlWng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "num_classes = 3  # Choose the desired number of classes\n",
        "y_classes = pd.qcut(y, q=num_classes, labels=False)\n",
        "\n",
        "# 1. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_classes, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Create a SMOTE object\n",
        "smote = SMOTE(random_state=42)  # You can adjust the random_state and other parameters\n",
        "\n",
        "# 3. Apply SMOTE to the training data\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# 4. Now you can train your model using the resampled data\n",
        "# ... (e.g., model.fit(X_train_resampled, y_train_resampled))"
      ],
      "metadata": {
        "id": "pEtXAcbtmrVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tree-Based Algorithm**"
      ],
      "metadata": {
        "id": "RfnqyKXXnw3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# 1. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Create a Decision Tree Regressor object\n",
        "tree_regressor = DecisionTreeRegressor(random_state=42)  # You can adjust hyperparameters\n",
        "\n",
        "# 3. Train the model\n",
        "tree_regressor.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions on the test set\n",
        "y_pred = tree_regressor.predict(X_test)\n",
        "\n",
        "# 5. Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)"
      ],
      "metadata": {
        "id": "Q56DP_VQnz-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overall Interpretation:**\n",
        "\n",
        "The combination of a relatively low MSE (31.59) and a very high R-squared (0.999977) suggests that your Decision Tree Regressor is performing exceptionally well in predicting Ether prices based on the data you've provided."
      ],
      "metadata": {
        "id": "Pzh83f06sIme"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  3.6. Data Reduction"
      ],
      "metadata": {
        "id": "12hbqF3xsKvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dimensionality Reduction**"
      ],
      "metadata": {
        "id": "TBvPpA0nsTYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# 1. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Standardize the features (important for PCA)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 3. Apply PCA\n",
        "pca = PCA(n_components=0.95)  # Keep 95% of variance\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# 4. Now you can train your model using the reduced features\n",
        "# ... (e.g., model.fit(X_train_pca, y_train))"
      ],
      "metadata": {
        "id": "1-SiSfPNsPyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Numerosity Reduction - original data<>smaller form**"
      ],
      "metadata": {
        "id": "HGDIjeJtvPjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# 1. Fit a linear regression model\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X, y)\n",
        "\n",
        "# 2. Store the model coefficients and intercept\n",
        "coefficients = regressor.coef_\n",
        "intercept = regressor.intercept_\n",
        "\n",
        "# 3. Use the model parameters to represent the data\n",
        "# ... (You can now use the coefficients and intercept to make predictions\n",
        "#      or to reconstruct an approximation of the original data)"
      ],
      "metadata": {
        "id": "1MmQqBESvRHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Exploratory Data Analysis (EDA)**"
      ],
      "metadata": {
        "id": "eA5GRTlVG75T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.1. Distribution Analysis:**"
      ],
      "metadata": {
        "id": "NBRCEc8OHK7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary Statistics:**"
      ],
      "metadata": {
        "id": "Y9k9BzkPHXqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_stats = df['Close'].describe()\n",
        "print(summary_stats)"
      ],
      "metadata": {
        "id": "aq8d_bIHHWZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Histogram:**"
      ],
      "metadata": {
        "id": "B0fPRTBXHxux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Histogram of the 'Close' price\n",
        "plt.hist(df['Close'], bins=20)\n",
        "plt.xlabel('Close Price')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Ether Closing Prices')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CBzX_JBiH1YQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Box Plot:**"
      ],
      "metadata": {
        "id": "gwGChS2UH-Tc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Box plot of the 'Volume' feature\n",
        "sns.boxplot(x=df['Volume'])\n",
        "plt.xlabel('Trading Volume')\n",
        "plt.title('Box Plot of Trading Volume')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V52O87D9IBQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.2. Bivariate Analysis:**"
      ],
      "metadata": {
        "id": "ZmhHLsVzIgEk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scatter Plot of Close Price vs. Volume:**\n",
        "\n",
        "This visualization helps you understand if trading volume has any influence on Ether's closing price."
      ],
      "metadata": {
        "id": "q37mAOG4Iw0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))  # Adjust figure size as needed\n",
        "plt.scatter(df['Volume'], df['Close'], alpha=0.5)  # Alpha for transparency\n",
        "plt.xlabel('Volume')\n",
        "plt.ylabel('Close Price')\n",
        "plt.title('Scatter Plot of Close Price vs. Volume')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t0ctNWxrIwA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bar Chart of Price Change vs. Day of the Week:**"
      ],
      "metadata": {
        "id": "Jp0utTHWJN7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming 'df' has a 'Date' column:\n",
        "df['date'] = pd.to_datetime(df['date'])  # Convert to datetime if needed\n",
        "df['DayOfWeek'] = df['date'].dt.dayofweek  # 0: Monday, 6: Sunday\n",
        "df['PriceChange'] = df['Close'] - df['Open']  # Daily price change\n",
        "\n",
        "# Create the bar chart\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x='DayOfWeek', y='PriceChange', data=df)\n",
        "plt.xlabel('Day of the Week (0: Monday, 6: Sunday)')\n",
        "plt.ylabel('Average Price Change')\n",
        "plt.title('Bar Chart of Price Change vs. Day of the Week')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PYttYD3mJcZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**4.3. Multivariate Analysis & Feature Relationships:**"
      ],
      "metadata": {
        "id": "0GMVQPjBJl6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Heatmaps:**"
      ],
      "metadata": {
        "id": "jzGFBTcnXhXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "numerical_features = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "correlation_matrix = df[numerical_features].corr()\n",
        "plt.figure(figsize=(10, 8))  # Adjust figure size as needed\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b7XuAN-nXpUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pair Plots:**"
      ],
      "metadata": {
        "id": "L4AnPUCIXyP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Assuming 'df' is your DataFrame and 'numerical_features' is a list of numerical columns:\n",
        "\n",
        "sns.pairplot(df[numerical_features])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z2WC6P_VXzKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.4. Temporal Analysis:**"
      ],
      "metadata": {
        "id": "DFSLzl0yYg1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temporal Analysis, which involves studying data patterns and trends over time. This is particularly crucial for your Ether price prediction project, as financial time series data often exhibits temporal dependencies and seasonality."
      ],
      "metadata": {
        "id": "EJFuijGdYnTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Techniques for Temporal Analysis:**"
      ],
      "metadata": {
        "id": "m4TsV5cuY8aG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Line Plots:** Line plots are fundamental for visualizing time series data. They show how a variable changes over time, with the x-axis representing time and the y-axis representing the variable's value."
      ],
      "metadata": {
        "id": "cNSlhLtGY9wO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'df' is your DataFrame with a 'Date' column and a 'Close' column:\n",
        "plt.plot(df['date'], df['Close'])\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.title('Ether Price Over Time')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wnvSom6NY4lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Moving Averages:** Moving averages smooth out short-term fluctuations in time series data, revealing longer-term trends."
      ],
      "metadata": {
        "id": "NxDS4WD5ZLSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate a 7-day moving average\n",
        "df['MA_7'] = df['Close'].rolling(window=7).mean()\n",
        "\n",
        "# Plot the moving average along with the original data\n",
        "plt.plot(df['date'], df['Close'], label='Original')\n",
        "plt.plot(df['date'], df['MA_7'], label='7-Day MA')\n",
        "plt.xlabel('date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.title('Ether Price with Moving Average')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4Amq6BRRZU0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Seasonality Analysis:** Seasonality refers to recurring patterns in data at regular intervals (e.g., daily, weekly, monthly). You can use techniques like autocorrelation and seasonal decomposition to identify and analyze seasonality."
      ],
      "metadata": {
        "id": "bodiCKCjZeER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install statsmodels\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# Perform seasonal decomposition\n",
        "result = seasonal_decompose(df['Close'], model='additive', period=7)  # Adjust period as needed\n",
        "\n",
        "# Plot the components (trend, seasonal, residual)\n",
        "result.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "084fz1m1ZnJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Time-Based Features:** You can engineer new features based on time, such as day of the week, month, or year, to capture temporal patterns."
      ],
      "metadata": {
        "id": "_2rynymtZwfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['DayOfWeek'] = df['date'].dt.dayofweek  # 0: Monday, 6: Sunday\n",
        "df['Month'] = df['date'].dt.month"
      ],
      "metadata": {
        "id": "hyVbIoYFZoV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.5. Categorical Data Analysis:**"
      ],
      "metadata": {
        "id": "R5GfurpNaBpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Day of the Week Analysis:**\n",
        "\n",
        "To analyze patterns in price changes or trading volume across different days."
      ],
      "metadata": {
        "id": "d6oMSTXEbTTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**a. Bar Chart of Price Change vs. Day of the Week:**"
      ],
      "metadata": {
        "id": "WS0hcaIjaG7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df['PriceChange'] = df['Close'] - df['Open']  # Daily price change\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x='DayOfWeek', y='PriceChange', data=df)\n",
        "plt.xlabel('Day of the Week (0: Monday, 6: Sunday)')\n",
        "plt.ylabel('Average Price Change')\n",
        "plt.title('Bar Chart of Price Change vs. Day of the Week')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ED6RVSL8bE15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b. Pie Chart of Trading Volume by Day of the Week:**"
      ],
      "metadata": {
        "id": "6KLACBKtbINe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "volume_by_day = df.groupby('DayOfWeek')['Volume'].sum()\n",
        "plt.pie(volume_by_day.values, labels=volume_by_day.index, autopct='%1.1f%%')\n",
        "plt.title('Pie Chart of Trading Volume by Day of the Week')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yLT85IDzbgW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Price Movement Categories:**\n",
        "\n",
        "we can create categorical features based on price movements to visualize their distribution."
      ],
      "metadata": {
        "id": "vqgvEsECbk_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**a. Creating Price Movement Categories:**"
      ],
      "metadata": {
        "id": "dxl0W9GvbrpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['PriceMovement'] = pd.cut(df['PriceChange'], bins=[-float('inf'), 0, float('inf')], labels=['Down', 'Up'])"
      ],
      "metadata": {
        "id": "G4eEJv9hbqoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b. Bar Chart of Price Movement Frequencies:**"
      ],
      "metadata": {
        "id": "aqSaAAt9b6jj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "price_movement_counts = df['PriceMovement'].value_counts()\n",
        "plt.bar(price_movement_counts.index, price_movement_counts.values)\n",
        "plt.xlabel('Price Movement')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Bar Chart of Price Movement Frequencies')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5f32tVLWb95B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.6. Dimensionality Reduction Visualization (PCA): Reduce data dimensions and visualize using PCA.**\n"
      ],
      "metadata": {
        "id": "wf28HKEzcGjz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dimensionality Reduction Visualization using Principal Component Analysis (PCA)** is a technique that can help you reduce the number of features in your dataset while preserving as much variance as possible. This can be useful for visualization, as it allows you to plot data in a lower-dimensional space (e.g., 2D or 3D) while still capturing the essential relationships between variables.\n",
        "\n",
        "**Steps for PCA Visualization:**\n",
        "\n",
        "1. **Data Preparation:**\n",
        "\n",
        " * Select the numerical features you want to include in the PCA.\n",
        "\n",
        " * Standardize the features to have zero mean and unit variance. This is important for PCA to work effectively, as it is sensitive to the scales of different features.\n",
        "2. **Apply PCA:**\n",
        "\n",
        " * Import the PCA class from sklearn.decomposition.\n",
        "\n",
        " * Create a PCA object, specifying the desired number of components (e.g., n_components=2 for 2D visualization).\n",
        "\n",
        " * Fit the PCA object to your standardized data.\n",
        "\n",
        " * Transform your data using the fitted PCA object to obtain the principal components.\n",
        "\n",
        "3. **Visualization:**\n",
        "\n",
        " * Create a scatter plot using the principal components as the x and y axes.\n",
        "\n",
        " * Optionally, color the points based on a categorical variable (e.g., price movement categories) to see if there are any patterns or clusters."
      ],
      "metadata": {
        "id": "fjmHb5KQcMZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Assuming 'df' is your DataFrame and 'numerical_features' is a list of numerical columns:\n",
        "\n",
        "# 1. Data Preparation\n",
        "X = df[numerical_features]\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 2. Apply PCA\n",
        "pca = PCA(n_components=2)  # Reduce to 2 dimensions\n",
        "principal_components = pca.fit_transform(X_scaled)\n",
        "\n",
        "# 3. Visualization\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(principal_components[:, 0], principal_components[:, 1])\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA Visualization')\n",
        "plt.show()\n",
        "\n",
        "# Optional: Color points based on a categorical variable\n",
        "# Assuming 'category_column' is a categorical column in your DataFrame:\n",
        "# plt.scatter(principal_components[:, 0], principal_components[:, 1], c=df['category_column'])"
      ],
      "metadata": {
        "id": "3RGG7WHRdWPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.7. Statistical & Hypothesis Tests:**"
      ],
      "metadata": {
        "id": "3umerFZgdaEf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. T-Test (Example: Comparing Average 'Close' Price on Weekdays vs. Weekends)**"
      ],
      "metadata": {
        "id": "GUkiVHM5s1GQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Create groups for weekdays (0-4) and weekends (5-6)\n",
        "# Assuming 'DayOfWeek' is already created (0: Monday, 6: Sunday)\n",
        "weekday_prices = df[df['DayOfWeek'].isin([0, 1, 2, 3, 4])]['Close']\n",
        "weekend_prices = df[df['DayOfWeek'].isin([5, 6])]['Close']\n",
        "\n",
        "# Perform the t-test\n",
        "t_statistic, p_value = stats.ttest_ind(weekday_prices, weekend_prices)\n",
        "\n",
        "print(\"T-statistic:\", t_statistic)\n",
        "print(\"P-value:\", p_value)"
      ],
      "metadata": {
        "id": "tK6AmG3fw8Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Chi-Square Test (Example: Relationship between Price Movement and Day of the Week)**"
      ],
      "metadata": {
        "id": "MZPj5eJvxDPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Assuming 'PriceMovement' is already created ('Up' or 'Down')\n",
        "# Create a contingency table\n",
        "contingency_table = pd.crosstab(df['DayOfWeek'], df['PriceMovement'])\n",
        "\n",
        "# Perform the chi-square test\n",
        "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "print(\"Chi-square statistic:\", chi2)\n",
        "print(\"P-value:\", p)"
      ],
      "metadata": {
        "id": "QaEnb4tNxEf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.8. Complex Data Type Visualization:**"
      ],
      "metadata": {
        "id": "IZOkgorExr5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have text data related to Ether (e.g., news articles, social media posts), you can use word clouds and image-based visualizations to gain insights into the distribution of words and topics."
      ],
      "metadata": {
        "id": "RBUo6-baxwiG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example: Word Cloud Visualization**"
      ],
      "metadata": {
        "id": "2YNzm44syC0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordcloud\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample text data (replace with your actual text data)\n",
        "text = \"Ether cryptocurrency blockchain technology digital decentralized finance\"\n",
        "\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=STOPWORDS).generate(text)\n",
        "\n",
        "# Display the generated image:\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UUJrt-D3yGwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example: Image with Distribution Insights (Using a Histogram)**"
      ],
      "metadata": {
        "id": "EzcyFxYLyYJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Sample image data (replace with your actual image data)\n",
        "image = np.random.randint(0, 256, size=(100, 100, 3), dtype=np.uint8)\n",
        "\n",
        "# Calculate color histograms for each channel (Red, Green, Blue)\n",
        "color = ('r', 'g', 'b')\n",
        "for i, col in enumerate(color):\n",
        "    histr = cv2.calcHist([image], [i], None, [256], [0, 256])\n",
        "    plt.plot(histr, color=col)\n",
        "    plt.xlim([0, 256])\n",
        "\n",
        "plt.title('Color Histogram')\n",
        "plt.xlabel('Pixel Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NHOfiNa3yEU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.9. Geospatial Analysis:**"
      ],
      "metadata": {
        "id": "jzRvQVOsyx20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vYOSNaw7y3Pg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.10. Segmentation Analysis:**"
      ],
      "metadata": {
        "id": "Nk9GtQhmzRIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Segmentation analysis involves dividing your data into groups or segments based on shared characteristics. This can help you uncover hidden patterns and gain a deeper understanding of different customer or data point behaviors."
      ],
      "metadata": {
        "id": "MPpntSuGzYKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Clustering Techniques:**\n",
        "\n",
        "We'll use clustering algorithms to identify natural groupings within your data. Popular clustering methods include:\n",
        "\n",
        "**K-Means:** Partitions data into k clusters based on distance from cluster centroids.\n",
        "\n",
        "**Hierarchical Clustering:** Builds a hierarchy of clusters based on similarity.\n",
        "\n",
        "**DBSCAN:** Groups data points based on density."
      ],
      "metadata": {
        "id": "kfZpVxa_zlUh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example: K-Means Clustering**"
      ],
      "metadata": {
        "id": "YG2sC6iKz4t6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Select features for clustering (e.g., 'Close', 'Volume')\n",
        "features_for_clustering = ['Close', 'Volume']\n",
        "X = df[features_for_clustering]\n",
        "\n",
        "# Standardize the features (important for K-Means)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Determine the optimal number of clusters (e.g., using the elbow method)\n",
        "# ... (code for elbow method)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "num_clusters = 3  # Replace with the optimal number of clusters\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "df['Cluster'] = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Analyze the characteristics of each cluster\n",
        "# ... (e.g., calculate average 'Close' price, volume, etc. for each cluster)\n",
        "\n",
        "# Visualize the clusters (e.g., using scatter plots)\n",
        "# ... (code for visualization)"
      ],
      "metadata": {
        "id": "gh6UevPSz-OD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Segmentation Based on Time-Based Features:**\n",
        "\n",
        "We can also segment your data based on time-based features like day of the week, hour of the day, or month. This can help you identify patterns in price movements or trading volume during specific time periods."
      ],
      "metadata": {
        "id": "RF61I9Yn0Elk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example: Segmenting by Day of the Week**"
      ],
      "metadata": {
        "id": "vOs9I10i0Nrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average 'Close' price for each day of the week\n",
        "average_price_by_day = df.groupby('DayOfWeek')['Close'].mean()\n",
        "print(average_price_by_day)\n",
        "\n",
        "# Analyze and compare the average prices for different days\n",
        "# ... (e.g., identify days with higher or lower average prices)\n",
        "\n",
        "# Visualize the segments (e.g., using bar charts)\n",
        "# ... (code for visualization)"
      ],
      "metadata": {
        "id": "L91PS_2X0B9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Segmentation Based on Price Movement:**\n",
        "\n",
        "You can segment your data based on the direction of price movement (up or down) to analyze patterns in different market conditions."
      ],
      "metadata": {
        "id": "yFYxSjuh0UqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example: Segmenting by Price Movement**"
      ],
      "metadata": {
        "id": "eWluPLdE0lzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average volume for 'Up' and 'Down' price movements\n",
        "average_volume_by_movement = df.groupby('PriceMovement', observed=True)['Volume'].mean()\n",
        "print(average_volume_by_movement)\n",
        "\n",
        "# Analyze and compare the average volumes for different price movements\n",
        "# ... (e.g., see if volume is higher during uptrends or downtrends)\n",
        "\n",
        "# Visualize the segments (e.g., using bar charts)\n",
        "# ... (code for visualization)"
      ],
      "metadata": {
        "id": "dKfW-ZVT0mpQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "authorship_tag": "ABX9TyPu8IT/1qs8XkDSzlndm2kK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}