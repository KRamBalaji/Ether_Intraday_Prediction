{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KRamBalaji/Ether_Intraday_Prediction/blob/main/ML_Assessment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORLuzQkceN2_"
      },
      "source": [
        "# Project -"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYQ2L_eheTgQ"
      },
      "source": [
        "## **1. Define Problem Statement-**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK3WNK9ksvK8"
      },
      "source": [
        "**Problem Statement:**\n",
        "\n",
        "> To predict IntraDay Ether (ETH) prices using machine learning, enabling informed trading decisions and potential profit maximization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRFnRnA8tsYk"
      },
      "source": [
        "### 1.1. Industry & Problem Type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt6iZKsQtCpU"
      },
      "source": [
        "\n",
        "\n",
        "* **Industry:** Cryptocurrency Trading/Financial Markets\n",
        "* **Problem Type:** This is primarily a **supervised learning** problem. We'll use historical Ether price data (labeled data) to train a model that can predict future prices. There might be elements of **time series analysis** involved as well due to the temporal nature of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7S_eL0Utova"
      },
      "source": [
        "### 1.2. Business Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I81URWQxtZUq"
      },
      "source": [
        "\n",
        "\n",
        "* **Why this problem?** Accurate prediction of Ether prices can provide a significant advantage in trading, allowing for better timing of buy and sell orders.\n",
        "* **Desired Outcome:** Develop a model capable of predicting IntraDay Ether prices with a certain level of accuracy (to be defined in the evaluation metrics section) to assist in trading decisions and potentially increase profitability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDpuFh_ztirf"
      },
      "source": [
        "### 1.3. Constraints & Limitations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCDAEnoztxai"
      },
      "source": [
        "* **Data Availability:** While historical Ether price data is generally available, obtaining high-quality, granular IntraDay data might require access to specific APIs or data providers.\n",
        "* **Computational Power:** Training complex machine learning models, especially with large datasets, may require significant computational resources. Depending on the chosen model and data size, we might need to consider cloud-based solutions like Google Colab for training.\n",
        "* **Obstacle:** The cryptocurrency market is highly volatile and influenced by various external factors. Achieving consistently accurate predictions can be challenging.\n",
        "* **Budget:** Depending on the data sources and computational resources required, there might be associated costs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ADaFXMFuBMf"
      },
      "source": [
        "### 1.4. Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59EnML1ZuEwn"
      },
      "source": [
        "* **Optimization Required:** We need to optimize the model for accuracy, precision, and potentially other metrics relevant to trading, such as minimizing false positives/negatives.\n",
        "* **KPIs Tracking:** Key performance indicators (KPIs) could include:\n",
        " * **Mean Absolute Error (MAE):** Measures the average absolute difference between predicted and actual prices.\n",
        " * **Root Mean Squared Error (RMSE):** Gives a higher weight to large errors.\n",
        " * **R-squared:** Indicates the proportion of variance in the dependent variable (price) explained by the model.\n",
        " * **Profitability/Returns:** Simulating trading strategies based on model predictions to assess potential gains.\n",
        "\n",
        "* **Required Testing:** We'll need to rigorously test the model on unseen data (a holdout set or through cross-validation) to ensure its generalization ability and avoid overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M6Mb3NjuiDW"
      },
      "source": [
        "### 1.5. Target Audience Relevancy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShPbq0HNum6-"
      },
      "source": [
        "* **Model Prediction Usage:** The primary target audience would be cryptocurrency traders or investors.\n",
        "* **Speed of Prediction:** For IntraDay trading, prediction speed is crucial. The model should be able to generate predictions quickly to enable timely trading decisions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWXVnkScuw63"
      },
      "source": [
        "### 1.6. Data Availability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prSkbIJSuzEW"
      },
      "source": [
        "* **Ease of Data Collection:** As mentioned earlier, historical Ether price data is readily available, but obtaining high-quality, granular IntraDay data might require some effort.\n",
        "* **Necessary Features:** Potential features could include:\n",
        " * Historical price data (Open, High, Low, Close)\n",
        " * Trading volume\n",
        " * Market sentiment (derived from social media or news articles)\n",
        " * Technical indicators (e.g., moving averages, RSI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faE2xX9Su09O"
      },
      "source": [
        "### 1.7. Scope of the Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MCWAfwTvGQk"
      },
      "source": [
        "* **Capabilities:** The solution aims to provide:\n",
        " * Accurate IntraDay Ether price predictions.\n",
        " * Potential trading signals (buy/sell) based on predictions.\n",
        " * Visualization of predicted prices and historical data.\n",
        "* **Expectations:** It's important to note that this solution is intended to be a tool to assist in trading decisions, not a guaranteed money-making machine. The cryptocurrency market is inherently unpredictable, and no model can perfectly predict future prices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAuZz7J1vWZp"
      },
      "source": [
        "### 1.8. Deployment Considerations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CemF3VzyveRr"
      },
      "source": [
        "* **Platform:** Google Colab could be used for development and initial deployment.\n",
        "Integration:\n",
        " * For wider accessibility, we could consider deploying the model as a web application or integrating it with a trading platform.\n",
        " * An API could be developed to provide predictions to other applications or systems.\n",
        "* **IntraDay Prices:** We'll need to ensure the data pipeline can continuously fetch and update IntraDay Ether prices to keep the model's predictions relevant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJgAcYVBvzdW"
      },
      "source": [
        "## **2. Data Collection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACcuWSx6wTee"
      },
      "source": [
        "### 2.1. Source Identification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2rV2JKrx0hA"
      },
      "source": [
        "* **Reliable Sources:** For this project, we can consider the following source:\n",
        " * **Kaggle:** Kaggle is an online community platform for data scientists and machine learning enthusiasts. It's owned by Google and serves several key purposes:\n",
        "   * **Datasets:** Kaggle provides a vast repository of publicly available datasets that users can explore, download, and use for their projects. You can find datasets on various topics, from image recognition to natural language processing.\n",
        "   *  **Notebooks:** Kaggle offers cloud-based Jupyter Notebooks, allowing users to write and execute code directly on the platform. These notebooks are integrated with GPUs and TPUs for accelerated computation.\n",
        "   * **Community:** Kaggle fosters a vibrant community of data scientists and machine learning practitioners. Users can share their work, collaborate on projects, ask questions, and learn from each other through forums and discussions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5tx00hyx07Y"
      },
      "source": [
        "### 2.2. Data Volume Required"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmcFBjW0x5eF"
      },
      "source": [
        "* **Sufficient Data:** The amount of data required depends on the complexity of the model and the desired accuracy. Generally, more data is better for training machine learning models, especially for time series analysis. For this project, aiming for at least a few years of historical IntraDay data (hourly or even more granular) would be ideal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMiViXDPx6Hs"
      },
      "source": [
        "### 2.3. Data Types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQu5vFnUx-eO"
      },
      "source": [
        "* **Labeled Data:** We'll be primarily working with labeled data, as historical Ether price data comes with timestamps and corresponding prices. These prices will serve as our target variable for training the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFsv0zieyBJH"
      },
      "source": [
        "### 2.4. Data Quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb5s9KAkyCnp"
      },
      "source": [
        "* **Data Cleaning:** Addressing data quality issues is crucial. We need to:\n",
        " * **Handle Missing Values:** Decide on a strategy for dealing with missing data points, such as imputation or removal.\n",
        " * **Outlier Detection and Treatment:** Identify and handle outliers that could skew the model's training.\n",
        " * **Data Consistency:** Ensure data is in a consistent format and units across all sources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yQMJWcnyEM6"
      },
      "source": [
        "### 2.5. Data Relevancy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2trvsO0QyGAB"
      },
      "source": [
        "* **Feature Selection:** We'll need to carefully select features that are relevant to Ether price prediction. This might involve:\n",
        " * **Historical Price Data:** Open, High, Low, Close prices.\n",
        " * **Trading Volume:** Indicator of market activity.\n",
        " * **Technical Indicators:** Moving averages, RSI, MACD, etc.\n",
        " * **Market Sentiment:** Data derived from social media, news articles, or dedicated sentiment analysis APIs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIuQx2ARyHPT"
      },
      "source": [
        "### 2.6. Temporal Considerations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njDdTVv0yJ1g"
      },
      "source": [
        "* **Time Series Nature:** Ether price data is inherently temporal, and we need to account for this:\n",
        " * **Seasonality:** Analyze for any seasonal patterns in price movements.\n",
        " * **Time-Based Features:** Consider incorporating time-based features like day of the week, hour of the day, etc., as potential predictors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i84Gl4PVyMf9"
      },
      "source": [
        "### 2.7. Legal and Ethical Concerns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cePpuDNbyPYV"
      },
      "source": [
        "* **Data Usage Rights:** Ensure you have the right to use the data from the chosen sources. Check API terms of service and website scraping policies.\n",
        "* **Privacy:** If incorporating personal or sensitive data (e.g., user data), ensure compliance with privacy regulations and data protection practices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLlz0a-FyRQb"
      },
      "source": [
        "### 2.8. Sampling Strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXYTSCHdyZdT"
      },
      "source": [
        "* **Sampling:** While having more data is generally better, sampling can be useful for initial model development or when dealing with massive datasets. You can consider techniques like random sampling or stratified sampling if appropriate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDqdkNGxyU7x"
      },
      "source": [
        "### 2.9. Data Privacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puNYrq_GygNB"
      },
      "source": [
        "* **Data Anonymization:** If using sensitive data, consider anonymization techniques to protect privacy. However, in this case, we'll primarily be working with publicly available market data, so this might not be a major concern."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ovmpkt_gycZU"
      },
      "source": [
        "### 2.10. Data Collection Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0GAAjICyTGm"
      },
      "source": [
        "* **APIs:** Use libraries like requests in Python to interact with exchange APIs and fetch data.\n",
        "* **Web Scraping:** Libraries like Beautiful Soup and Scrapy can be used for web scraping. However, exercise caution and respect website robots.txt rules.\n",
        "* **Data Handling:** Use libraries like pandas for data manipulation and storage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoMuYyXYylNJ"
      },
      "source": [
        "### 2.11. Data Versioning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRGmUMkaymzi"
      },
      "source": [
        "* **Version Control:** Implement version control using Git to track changes to your dataset and code. This is essential for reproducibility and collaboration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mYyPREYyqfm"
      },
      "source": [
        "### 2.12. Continuous Data Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eU7v26Rysi0"
      },
      "source": [
        "* **Data Pipelines:** Design a data pipeline to regularly fetch and update your dataset with fresh IntraDay Ether prices. This could involve scheduling scripts to run at specific intervals or using real-time data streams if available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HZ2To1z2Xqp"
      },
      "source": [
        "## **3. Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0JiDeI0r2zq5"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from numpy import loadtxt\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from matplotlib import rcParams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfOUH8D_20ai",
        "outputId": "1f509373-603d-4266-ca06-a17332b0ed46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Importing the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/ether_intraday_prices.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "PIa4kc6V26r8",
        "outputId": "4736d014-74a7-46ee-a135-20b5ad159b9f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  date    Open    High     Low   Close    Volume\n",
              "0  2017-08-17 04:00:00  301.13  301.13  298.00  298.00   5.80167\n",
              "1  2017-08-17 04:15:00  298.00  300.80  298.00  299.39  31.44065\n",
              "2  2017-08-17 04:30:00  299.39  300.79  299.39  299.60  52.93579\n",
              "3  2017-08-17 04:45:00  299.60  302.57  299.60  301.61  35.49066\n",
              "4  2017-08-17 05:00:00  301.61  302.57  300.95  302.01  81.69235"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dd3b6bba-9969-497e-ad86-03478ef1a099\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2017-08-17 04:00:00</td>\n",
              "      <td>301.13</td>\n",
              "      <td>301.13</td>\n",
              "      <td>298.00</td>\n",
              "      <td>298.00</td>\n",
              "      <td>5.80167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2017-08-17 04:15:00</td>\n",
              "      <td>298.00</td>\n",
              "      <td>300.80</td>\n",
              "      <td>298.00</td>\n",
              "      <td>299.39</td>\n",
              "      <td>31.44065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2017-08-17 04:30:00</td>\n",
              "      <td>299.39</td>\n",
              "      <td>300.79</td>\n",
              "      <td>299.39</td>\n",
              "      <td>299.60</td>\n",
              "      <td>52.93579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2017-08-17 04:45:00</td>\n",
              "      <td>299.60</td>\n",
              "      <td>302.57</td>\n",
              "      <td>299.60</td>\n",
              "      <td>301.61</td>\n",
              "      <td>35.49066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2017-08-17 05:00:00</td>\n",
              "      <td>301.61</td>\n",
              "      <td>302.57</td>\n",
              "      <td>300.95</td>\n",
              "      <td>302.01</td>\n",
              "      <td>81.69235</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd3b6bba-9969-497e-ad86-03478ef1a099')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dd3b6bba-9969-497e-ad86-03478ef1a099 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dd3b6bba-9969-497e-ad86-03478ef1a099');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cdde4985-bab8-43c2-861b-c2e8c58821d4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cdde4985-bab8-43c2-861b-c2e8c58821d4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cdde4985-bab8-43c2-861b-c2e8c58821d4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Dataset First\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXMon0kx3EX0"
      },
      "source": [
        "### 3.1. Handling Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKrzTyAX3Hn9",
        "outputId": "0e3cd2b9-cbaa-49ab-d350-b17884b193a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date      0\n",
            "Open      0\n",
            "High      0\n",
            "Low       0\n",
            "Close     0\n",
            "Volume    0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSPa5huW9Xnt"
      },
      "source": [
        "### 3.2. Handling Outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyjbSlwCPShB"
      },
      "source": [
        "**Using Standard Deviation in Symmetric Curve**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "togf5EWk9f9n",
        "outputId": "cd49da53-bd0c-4cf0-863a-7e3573a06185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "147243    4653.36\n",
            "147649    4642.45\n",
            "147650    4684.19\n",
            "147651    4688.29\n",
            "147652    4689.38\n",
            "           ...   \n",
            "149931    4648.60\n",
            "149933    4655.00\n",
            "149934    4656.21\n",
            "149935    4660.58\n",
            "149936    4644.00\n",
            "Name: Close, Length: 644, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def find_outliers_sd(data, threshold=3):\n",
        "\n",
        "  # Calculate the mean and standard deviation\n",
        "  data_mean = np.mean(data)\n",
        "  data_std = np.std(data)\n",
        "\n",
        "  # Identify outliers\n",
        "  outliers = data[(data < data_mean - threshold * data_std) |\n",
        "                  (data > data_mean + threshold * data_std)]\n",
        "\n",
        "  return outliers\n",
        "\n",
        "# Example usage:\n",
        "# Assuming 'df' is your pandas DataFrame and 'column_name' is the column\n",
        "# containing the data you want to analyze:\n",
        "\n",
        "outliers = find_outliers_sd(df['Close'])\n",
        "\n",
        "# To see the output, run the code\n",
        "print(outliers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxpZoVAIPZTL"
      },
      "source": [
        "**Using IQR in skew-symmetric Curve**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arS0ACpE-nbU",
        "outputId": "fd15fb8d-f0ba-45e2-e715-5fb8d81b7fea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "130132    3979.19\n",
            "130213    3974.73\n",
            "130214    4009.39\n",
            "130215    4020.00\n",
            "130216    4062.43\n",
            "           ...   \n",
            "152454    3980.98\n",
            "152455    3982.34\n",
            "152456    3980.95\n",
            "152457    3981.37\n",
            "152458    3973.78\n",
            "Name: Close, Length: 6141, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def find_outliers_iqr(data):\n",
        "  # Calculate quantiles\n",
        "  q1 = np.quantile(data, 0.25)\n",
        "  q3 = np.quantile(data, 0.75)\n",
        "\n",
        "  # Calculate IQR\n",
        "  iqr = q3 - q1\n",
        "\n",
        "  # Define upper and lower bounds\n",
        "  upper_bound = q3 + 1.5 * iqr\n",
        "  lower_bound = q1 - 1.5 * iqr\n",
        "\n",
        "  # Identify outliers\n",
        "  outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
        "\n",
        "  return outliers\n",
        "\n",
        "# Assuming 'dataset' is your DataFrame and 'Close' is the column of interest:\n",
        "outliers = find_outliers_iqr(df['Close'])\n",
        "\n",
        "# Display the outliers\n",
        "print(outliers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o95FI30cPc0G"
      },
      "source": [
        "**Using Outlier Insensitive Algorithms.i.e. SVM, KNN, Decision Tree**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo5LcV_7-ziU",
        "outputId": "c6ecde6d-5862-4f4d-8e71-ab27bed607f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Support Vector Regression (SVR):\n",
            "  Mean Squared Error (MSE): 29512.387855455163\n",
            "  R-squared (R2): 0.979000938527319\n",
            "K-Nearest Neighbors (KNN):\n",
            "  Mean Squared Error (MSE): 463.20530405259603\n",
            "  R-squared (R2): 0.9996704137699087\n",
            "Decision Tree Regression:\n",
            "  Mean Squared Error (MSE): 31.58908164538165\n",
            "  R-squared (R2): 0.9999775233007039\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR  # For SVM\n",
        "from sklearn.neighbors import KNeighborsRegressor  # For KNN\n",
        "from sklearn.tree import DecisionTreeRegressor  # For Decision Tree\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "X = df[['Open', 'High', 'Low', 'Volume']]  # Features\n",
        "y = df['Close']  # Target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 1. Support Vector Regression (SVR)\n",
        "svr_model = SVR(kernel='rbf')  # You can explore other kernels like 'linear', 'poly'\n",
        "svr_model.fit(X_train, y_train)\n",
        "svr_predictions = svr_model.predict(X_test)\n",
        "\n",
        "# 2. K-Nearest Neighbors (KNN)\n",
        "knn_model = KNeighborsRegressor(n_neighbors=5)  # Experiment with different 'n_neighbors'\n",
        "knn_model.fit(X_train, y_train)\n",
        "knn_predictions = knn_model.predict(X_test)\n",
        "\n",
        "# 3. Decision Tree Regression\n",
        "dt_model = DecisionTreeRegressor(random_state=42)  # You can adjust hyperparameters\n",
        "dt_model.fit(X_train, y_train)\n",
        "dt_predictions = dt_model.predict(X_test)\n",
        "\n",
        "# Evaluate models\n",
        "def evaluate_model(predictions, model_name):\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    r2 = r2_score(y_test, predictions)\n",
        "    print(f\"{model_name}:\")\n",
        "    print(f\"  Mean Squared Error (MSE): {mse}\")\n",
        "    print(f\"  R-squared (R2): {r2}\")\n",
        "\n",
        "evaluate_model(svr_predictions, \"Support Vector Regression (SVR)\")\n",
        "evaluate_model(knn_predictions, \"K-Nearest Neighbors (KNN)\")\n",
        "evaluate_model(dt_predictions, \"Decision Tree Regression\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxemcrAj_Txz"
      },
      "source": [
        "### 3.3. Categorical Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb07ReGC_bIc"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHE8KhYL_bjt"
      },
      "source": [
        "### 3.4. Data Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5K3_iHB__hDM"
      },
      "source": [
        "**Standardisation**\n",
        "\n",
        "This approach is based on the assumption that data follows a normal distribution, where most values cluster around the mean, and outliers lie further away."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "t_hobEzo_3Su"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming 'dataset' is your DataFrame and you want to standardize\n",
        "# the features 'Open', 'High', 'Low', and 'Volume'\n",
        "features_to_standardize = ['Open', 'High', 'Low', 'Volume']\n",
        "X = df[features_to_standardize]\n",
        "y = df['Close']  # Target variable\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data and transform it\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data using the fitted scaler\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Now, X_train_scaled and X_test_scaled contain the standardized features\n",
        "# You can use these scaled features for training your models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6UbuwXUHy1S"
      },
      "source": [
        "**Normalisation**\n",
        "\n",
        "This method scales your data to a specific range, typically between 0 and 1, which can be beneficial for certain machine learning algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "92U8_0j-Hx2e"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def normalize_data(data):\n",
        "\n",
        "  # Create a MinMaxScaler object\n",
        "  scaler = MinMaxScaler()\n",
        "\n",
        "  # Fit the scaler to the data and transform it\n",
        "  normalized_data = scaler.fit_transform(data)\n",
        "\n",
        "  # If input was a pandas DataFrame, convert the output back to a DataFrame\n",
        "  if isinstance(data, pd.DataFrame):\n",
        "    normalized_data = pd.DataFrame(normalized_data, columns=data.columns, index=data.index)\n",
        "\n",
        "  return normalized_data\n",
        "\n",
        "\n",
        "columns_to_normalize = ['Open', 'High', 'Low', 'Volume']\n",
        "df[columns_to_normalize] = normalize_data(df[columns_to_normalize])\n",
        "\n",
        "# To see the output, run the code.\n",
        "# print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jVaoP6ZMdiQ"
      },
      "source": [
        "**Robust Scaler**\n",
        "\n",
        "a data transformation technique that's particularly useful when your data contains outliers. Unlike standard scaling (using mean and standard deviation), the Robust Scaler is less sensitive to extreme values.\n",
        "\n",
        "**How it Works:**\n",
        "\n",
        "The Robust Scaler removes the median and scales the data according to the quantile range (IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th percentile) and the 3rd quartile (75th percentile) of the data. This makes it robust to outliers, as the scaling is based on the more stable IQR rather than the potentially skewed mean and standard deviation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "oGaBIRFnIKfC"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "def robust_scale_data(data):\n",
        "\n",
        "  # Create a RobustScaler object\n",
        "  scaler = RobustScaler()\n",
        "\n",
        "  # Fit the scaler to the data and transform it\n",
        "  scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "  # If input was a pandas DataFrame, convert the output back to a DataFrame\n",
        "  if isinstance(data, pd.DataFrame):\n",
        "    scaled_data = pd.DataFrame(scaled_data, columns=data.columns, index=data.index)\n",
        "\n",
        "  return scaled_data\n",
        "\n",
        "\n",
        "columns_to_scale = ['Open', 'High', 'Low', 'Volume']\n",
        "df[columns_to_scale] = robust_scale_data(df[columns_to_scale])\n",
        "\n",
        "# To see the output, run the code\n",
        "# print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqypKB6iMiSs"
      },
      "source": [
        "**Sum of (median-observation)/IQR**\n",
        "\n",
        "* This method is a form of robust scaling that aims to center and scale your data using the median and the Interquartile Range (IQR), making it less sensitive to outliers.\n",
        "\n",
        "\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "\n",
        "\n",
        ">` X_scaled = (X - X_median) / IQR`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "FTOXuIhOMk8S"
      },
      "outputs": [],
      "source": [
        "def robust_scale_data_custom(data):\n",
        "\n",
        "    data_median = np.median(data)\n",
        "    q1 = np.quantile(data, 0.25)\n",
        "    q3 = np.quantile(data, 0.75)\n",
        "    iqr = q3 - q1\n",
        "\n",
        "    scaled_data = (data - data_median) / iqr\n",
        "\n",
        "    return scaled_data\n",
        "\n",
        "columns_to_scale = ['Open', 'High', 'Low', 'Volume']\n",
        "df['column_name_scaled'] = robust_scale_data_custom(df['Close'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CNKGQWKQXts"
      },
      "source": [
        "**Box-Cox Transformation**\n",
        "\n",
        "a powerful technique for transforming non-normal dependent variables into a more normal shape.\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "The Box-Cox transformation aims to stabilize variance and make the data more closely resemble a normal distribution. This is often desirable because many statistical methods assume normality for optimal performance.\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "The Box-Cox transformation is defined by the following formula:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "T(y) = (y^位 - 1) / 位   if 位 != 0\n",
        "T(y) = ln(y)          if 位 = 0\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "cOL0pslGP8xI"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "\n",
        "# Perform Box-Cox transformation\n",
        "transformed_data, lambda_value = stats.boxcox(df['Close'])\n",
        "\n",
        "# Add the transformed data to the DataFrame\n",
        "df['transformed_dependent_variable'] = transformed_data\n",
        "\n",
        "# To see the output, run the code.\n",
        "# print(df.head())\n",
        "# print(f\"Lambda value: {lambda_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2YuuhhjQmuI"
      },
      "source": [
        "**Gaussian Transformation**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "\n",
        "column_to_transform = 'Close'\n",
        "\n",
        "# Create a QuantileTransformer object with 'normal' output distribution\n",
        "qt = QuantileTransformer(output_distribution='normal', random_state=42)\n",
        "\n",
        "# Fit and transform the data\n",
        "df['column_name_gaussian'] = qt.fit_transform(df[[column_to_transform]])"
      ],
      "metadata": {
        "id": "GOt3i5UkeJ6-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logarithmic Transformation**"
      ],
      "metadata": {
        "id": "eVP_r5UiephS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "column_to_transform = 'Close'\n",
        "\n",
        "# Apply logarithmic transformation using NumPy's log function\n",
        "df[column_to_transform + '_log'] = np.log(df[column_to_transform])\n",
        "\n",
        "# Handle potential errors (e.g., log of 0 or negative values)\n",
        "# You might need to add a small constant to avoid log(0) errors\n",
        "df[column_to_transform + '_log'] = np.log(df[column_to_transform] + 1e-6)  # Example"
      ],
      "metadata": {
        "id": "Qaf-1HQGeuAn"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inverse Transformation**"
      ],
      "metadata": {
        "id": "OPFgA6CMe8mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Close_log_inverse'] = np.exp(df['Close_log'])"
      ],
      "metadata": {
        "id": "ljzTbVeBfC25"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Square Root Transformation**"
      ],
      "metadata": {
        "id": "sopU7R8OfetH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Close_sqrt'] = np.sqrt(df['Close'])"
      ],
      "metadata": {
        "id": "B6hG9Gu7filV"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exponential Transformation**"
      ],
      "metadata": {
        "id": "4m_xy0HTgZBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the 'Close' column\n",
        "df['Close_scaled'] = scaler.fit_transform(df[['Close']])\n",
        "\n",
        "# Apply exponential transformation to the scaled column\n",
        "df['Close_scaled_exp'] = np.exp(df['Close_scaled'])"
      ],
      "metadata": {
        "id": "lcu8c8Ltgav9"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "z9kY9ShNhRL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understanding Imbalanced Datasets**\n",
        "\n",
        "An imbalanced dataset occurs when one class (or category) has significantly more instances than another. In your case, if you're trying to predict price movements (e.g., up or down), you might find that one direction is much more common than the other in your historical data. This imbalance can lead to biased models that perform poorly on the minority class."
      ],
      "metadata": {
        "id": "tPy6sGgMjf0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross-Validation**"
      ],
      "metadata": {
        "id": "gTp5FIHvjiE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LinearRegression  # Example model\n",
        "from sklearn.metrics import mean_squared_error  # Example metric\n",
        "\n",
        "# 1. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Create your model\n",
        "model = LinearRegression()  # Example: using Linear Regression\n",
        "\n",
        "# 3. Perform cross-validation\n",
        "scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# 4. Evaluate the results\n",
        "# Convert negative MSE scores to positive\n",
        "mse_scores = -scores\n",
        "print(\"Cross-validation MSE scores:\", mse_scores)\n",
        "print(\"Average MSE:\", mse_scores.mean())\n",
        "\n",
        "# 5. Train the final model on the entire training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 6. Evaluate the final model on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "mse_test = mean_squared_error(y_test, y_pred)\n",
        "print(\"Test MSE:\", mse_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbFdrbZlhh3U",
        "outputId": "02db9017-a6a7-4c5a-ea76-be3c51a9567c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation MSE scores: [12.82658177 14.84659755 13.80088738 12.46887154 13.66572614]\n",
            "Average MSE: 13.521732876544888\n",
            "Test MSE: 13.499787319128627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Under Sampling**"
      ],
      "metadata": {
        "id": "Tbo1NZk-j4iH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Discretize the target variable 'y' (e.g., using qcut)\n",
        "num_classes = 3  # Choose the desired number of classes\n",
        "y_classes = pd.qcut(y, q=num_classes, labels=False)\n",
        "\n",
        "# 1. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_classes, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Create a RandomUnderSampler object\n",
        "undersampler = RandomUnderSampler(random_state=42)  # You can adjust the random_state\n",
        "\n",
        "# 3. Apply undersampling to the training data\n",
        "X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train, y_train)\n",
        "\n",
        "# 4. Now you can train your model using the resampled data\n",
        "# ... (e.g., model.fit(X_train_resampled, y_train_resampled))"
      ],
      "metadata": {
        "id": "6YTK2SvykFt0"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Over Sampling**"
      ],
      "metadata": {
        "id": "GhtMKhR-k54K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Discretize the target variable 'y' (e.g., using qcut)\n",
        "num_classes = 3  # Choose the desired number of classes\n",
        "y_classes = pd.qcut(y, q=num_classes, labels=False)\n",
        "\n",
        "# 1. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_classes, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Create a SMOTE object\n",
        "oversampler = SMOTE(random_state=42)  # You can adjust the random_state and other parameters\n",
        "\n",
        "# 3. Apply oversampling to the training data\n",
        "X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
        "\n",
        "# 4. Now you can train your model using the resampled data\n",
        "# ... (e.g., model.fit(X_train_resampled, y_train_resampled))"
      ],
      "metadata": {
        "id": "_83JPfQ2k3F7"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Synthetic Minority over Sampling Technique (SMOTE)**"
      ],
      "metadata": {
        "id": "qGIDuTDGlWng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "num_classes = 3  # Choose the desired number of classes\n",
        "y_classes = pd.qcut(y, q=num_classes, labels=False)\n",
        "\n",
        "# 1. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_classes, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Create a SMOTE object\n",
        "smote = SMOTE(random_state=42)  # You can adjust the random_state and other parameters\n",
        "\n",
        "# 3. Apply SMOTE to the training data\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# 4. Now you can train your model using the resampled data\n",
        "# ... (e.g., model.fit(X_train_resampled, y_train_resampled))"
      ],
      "metadata": {
        "id": "pEtXAcbtmrVA"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tree-Based Algorithm**"
      ],
      "metadata": {
        "id": "RfnqyKXXnw3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# 1. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Create a Decision Tree Regressor object\n",
        "tree_regressor = DecisionTreeRegressor(random_state=42)  # You can adjust hyperparameters\n",
        "\n",
        "# 3. Train the model\n",
        "tree_regressor.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions on the test set\n",
        "y_pred = tree_regressor.predict(X_test)\n",
        "\n",
        "# 5. Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q56DP_VQnz-7",
        "outputId": "c56544d7-0a25-4459-c362-3b2bce555c0e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 31.58908164538165\n",
            "R-squared: 0.9999775233007039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overall Interpretation:**\n",
        "\n",
        "The combination of a relatively low MSE (31.59) and a very high R-squared (0.999977) suggests that your Decision Tree Regressor is performing exceptionally well in predicting Ether prices based on the data you've provided."
      ],
      "metadata": {
        "id": "Pzh83f06sIme"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  3.6. Data Reduction"
      ],
      "metadata": {
        "id": "12hbqF3xsKvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dimensionality Reduction**"
      ],
      "metadata": {
        "id": "TBvPpA0nsTYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# 1. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Standardize the features (important for PCA)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 3. Apply PCA\n",
        "pca = PCA(n_components=0.95)  # Keep 95% of variance\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# 4. Now you can train your model using the reduced features\n",
        "# ... (e.g., model.fit(X_train_pca, y_train))"
      ],
      "metadata": {
        "id": "1-SiSfPNsPyL"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Numerosity Reduction - original data<>smaller form**"
      ],
      "metadata": {
        "id": "HGDIjeJtvPjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# ... (Assuming X and y are your features and target variable)\n",
        "\n",
        "# 1. Fit a linear regression model\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X, y)\n",
        "\n",
        "# 2. Store the model coefficients and intercept\n",
        "coefficients = regressor.coef_\n",
        "intercept = regressor.intercept_\n",
        "\n",
        "# 3. Use the model parameters to represent the data\n",
        "# ... (You can now use the coefficients and intercept to make predictions\n",
        "#      or to reconstruct an approximation of the original data)"
      ],
      "metadata": {
        "id": "1MmQqBESvRHL"
      },
      "execution_count": 42,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "authorship_tag": "ABX9TyO2sCBzmT9DRdfGeXYs3S5y",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}